{"name":"Cleansweep","tagline":"Ruby Utility for purging data in mysql","body":"Cleansweep is a utility for scripting purges using ruby in an\r\nefficient, low-impact manner on mysql innodb tables.  Based on the\r\nPercona `pt-archive` utility.\r\n\r\n## Installation\r\n\r\nAdd this line to your application's Gemfile:\r\n\r\n```ruby\r\ngem 'cleansweep'\r\n```\r\n\r\nAnd then execute:\r\n\r\n    $ bundle\r\n\r\nOr install it yourself as:\r\n\r\n    $ gem install cleansweep\r\n\r\n## How it works\r\n\r\nConsider the table:\r\n```sql\r\n    create table comments (\r\n       `id` int(11) primary key auto_increment,\r\n       `timestamp` datetime,\r\n       `account` int(11),\r\n       `liked` boolean,\r\n       key comments_on_account_timestamp(account, timestamp)\r\n    )\r\n```\r\nAssume there is an active record model for it:\r\n\r\n    class Comment < ActiveRecord::Base ; end\r\n\r\n### Purging by traversing an index\r\n\r\nThe most efficient way to work through a table is by scanning through\r\nan index one chunk at a time.\r\n\r\nLet's assume we want to purge Comments older than 1 month.  We can\r\nscan the primary key index or the `account`,`timestamp` index.  In\r\nthis case the latter will probably work better since we are evaluating\r\nthe timestamp for the purge.\r\n\r\n```ruby\r\n    r = CleanSweep::PurgeRunner.new model: Comment,\r\n                                    index: 'comments_on_account_timestamp' do | scope |\r\n        scope.where('timestamp < ?', 1.month.ago)\r\n    end\r\n```\r\n\r\nTo execute the purge, do:\r\n\r\n```ruby\r\n    count = r.execute_in_batches\r\n    puts \"Deleted #{count} rows\"\r\n```\r\n\r\nCheck what it will do:\r\n\r\n```ruby\r\n    r.print_queries($stdout)\r\n```\r\n\r\nThis will show you what it will do by printing out the three different\r\nstatements used:\r\n\r\n```sql\r\n    Initial Query:\r\n        SELECT  `id`,`account`,`timestamp`\r\n        FROM `comments` FORCE INDEX(comments_on_account_timestamp)\r\n        WHERE (timestamp < '2014-11-25 21:47:43')\r\n        ORDER BY `account` ASC,`timestamp` ASC\r\n        LIMIT 500\r\n    Chunk Query:\r\n        SELECT  `id`,`account`,`timestamp`\r\n        FROM `comments` FORCE INDEX(comments_on_account_timestamp)\r\n        WHERE (timestamp < '2014-11-25 21:47:43') AND (`account` > 0 OR (`account` = 0 AND `timestamp` > '2014-11-18 21:47:43'))\r\n    ORDER BY `account` ASC,`timestamp` ASC\r\n        LIMIT 500\r\n    Delete Statement:\r\n        DELETE\r\n        FROM `comments`\r\n        WHERE (`id` = 2)\r\n```\r\n\r\nIt does the initial statement once to get the first chunk of rows.\r\nThen it does subsequent queries starting at the index where the last\r\nchunk left off, thereby avoiding a complete index scan.  This works\r\nfine as long as you don't have rows with duplicate account id and\r\ntimestamps.  If you do, you'll possibly miss rows between chunks.\r\n\r\nTo avoid missing duplicates, you can traverse the index using only the\r\nfirst column with an inclusive comparator like `>=` instead of `>`.\r\nHere's what that would look like:\r\n\r\n```ruby\r\n    r = CleanSweep::PurgeRunner.new model:Comment,\r\n                                    index: 'comments_on_account_timestamp',\r\n                                    first_only: true do | scope |\r\n        scope.where('timestamp < ?', 1.month.ago)\r\n    end\r\n```\r\n\r\nThe chunk query looks like:\r\n\r\n```sql\r\n    SELECT  `id`,`account`,`timestamp`\r\n    FROM `comments` FORCE INDEX(comments_on_account_timestamp)\r\n    WHERE (timestamp < '2014-11-25 21:47:43') AND (`account` >= 0)\r\n    LIMIT 500\r\n```\r\n\r\nYou can scan the index in either direction.  To specify descending\r\norder, use the `reverse: true` option.\r\n\r\n### Copying rows from one table to another\r\n\r\nYou can use the same technique to copy rows from one table to another.\r\nSupport in CleanSweep is pretty minimal.  It won't _move_ rows, only\r\ncopy them, although it would be easy to fix this.  I used this to copy\r\nids into a temporary table which I then used to delete later.\r\n\r\nHere's an example that copies rows from the `Comment` model to the\r\n`ExpiredComment` model (`expired_comments`).  Comments older than one\r\nweek are copied.\r\n\r\n```ruby\r\n      copier = CleanSweep::PurgeRunner.new model: Comment,\r\n                                           index: 'comments_on_account_timestamp',\r\n                                           dest_model: ExpiredComment,\r\n                                           copy_only: true,\r\n                                           copy_columns: %w[liked] do do | model |\r\n        model.where('last_used_at < ?', 1.week.ago)\r\n      end\r\n```\r\n\r\nThe `copy_columns` option specifies additional columns to be inserted\r\ninto the `expired_comments` table.\r\n\r\nIf the column names are different in the destination table than in the\r\nsource table, you can specify a mapping with the `dest_columns` option\r\nwhich takes a map of source column name to destination name.\r\n\r\n### Deleting rows in another table\r\n\r\nWhat if you want to query one table and delete those rows in another?\r\nI needed this when I built a temporary table of account ids that\r\nreferenced deleted accounts.  I then wanted to delete rows in other\r\ntables that referenced those account ids.  To do that, specify a\r\n`dest_table` without specifying `copy_only` mode.  This will execute\r\nthe delete statement on the destination table without removing rows\r\nfrom the source table.\r\n\r\nHere's an example:\r\n\r\n```sql\r\n      create temporary table expired_metrics (\r\n           metric_id int,\r\n           account_id int,\r\n           primary key (account_id, metric_id)\r\n      EOF\r\n```\r\nThen run a job to pull account_id, metric_id into the expired metrics table:\r\n\r\n```ruby\r\ncopier = CleanSweep::PurgeRunner.new index: 'index_on_metric_account_id',\r\n                                     model: AccountMetric,\r\n                                     dest_model: ExpiredMetric,\r\n                                     copy_only: true) do | model |\r\n    model.where(\"last_used_at < ?)\", expiration_date)\r\nend\r\ncopier.execute_in_batches\r\n```\r\n\r\nNow create as many jobs as you need for the tables which refer to these metrics:\r\n\r\n```ruby\r\nCleanSweep::PurgeRunner.new(model: ExpiredMetric,\r\n                            index: 'PRIMARY',\r\n                            dest_model: Metric,\r\n                            dest_columns: { 'metric_id' => 'id'} ).execute_in_batches\r\n\r\nCleanSweep::PurgeRunner.new(model: ExpiredMetric,\r\n                            index: 'PRIMARY',\r\n                            dest_model: ChartMetric).execute_in_batches\r\n\r\nCleanSweep::PurgeRunner.new(model: ExpiredMetric,\r\n                            index: 'PRIMARY',\r\n                            dest_model: SystemMetric).execute_in_batches\r\n```\r\n\r\nThese will delete the expired metrics from all the tables that refer to them.\r\n\r\n### Watching the history list and replication lag\r\n\r\nYou can enter thresholds for the history list size and replication lag\r\nthat will be used to pause the purge if either of those values get\r\ninto an unsafe territory.  The script will pause for 5 minutes and\r\nonly start once the corresponding metric goes back down to 90% of the\r\nspecified threshold.\r\n\r\n### Logging and monitoring progress\r\n\r\nYou pass in a standard log instance to capture all running output.  By\r\ndefault it will log to your `ActiveRecord::Base` logger, or stdout if\r\nthat's not set up.\r\n\r\nIf you specify a reporting interval with the `report` option it will\r\nprint the status of the purge at that interval.  This is useful to\r\ntrack progress and assess the rate of deletion.\r\n\r\n### Joins and subqueries\r\n\r\nYou can add subqueries and joins to your query in the scope block, but\r\nbe careful.  The index and order clause may work against you if the\r\ntable you are joining with doesn't have good parity with the indexes\r\nin your target table.\r\n\r\n### Limitations\r\n\r\n* Only works for mysql (as far as I know).  I have only used it against 5.5.\r\n* Should work with ActiveRecord 3.* - 4.*.\r\n* Using a non-unique index risks missing duplicate rows unless you use the `first_only` option.\r\n* Using the `first_only` option risks rescanning many rows if you have many more duplicates than your\r\n  chunk size\r\n* An index is required but you should be able to run a purge without one.  It just means you're not\r\n  scanning the index in chunks.  This might be okay if you are deleting everything as you go along because\r\n  then you're not rescanning the rows.  It wouldn't require much to modify CleanSweep to support this\r\n  mode.\r\n\r\n### Other options\r\n\r\nThere are a number of other options you can use to tune the script.\r\nFor details look at the [API on the `PurgeRunner`\r\nclass](http://bkayser.github.io/cleansweep/rdoc/CleanSweep/PurgeRunner.html)\r\n\r\n### New Relic integration\r\n\r\nThe script requires the [New Relic](http://github.com/newrelic/rpm)\r\ngem.  It won't impact anyting if you don't have a New Relic account to\r\nreport to, but if you do use New Relic it is configured to show you\r\ndetailed metrics.\r\n\r\nIn order to see the data in New Relic your purge must be identified as\r\na background transaction.  If you are running in Resque or DelayedJob,\r\nit will automatically be tagged as such, but if you are just invoking\r\nyour purge directly, you'll need to tag it as a background\r\ntransaction.  The easy way to do that is shown in this example:\r\n\r\n```ruby\r\n    class Purge\r\n      include NewRelic::Agent::Instrumentation::ControllerInstrumentation\r\n      def run()\r\n         ...\r\n      end\r\n      add_transaction_tracer :run\r\n    end\r\n```\r\nAlso, I recommend turning off transaction traces for long\r\npurge jobs to reduce your memory footprint.\r\n\r\n## Testing\r\n\r\nTo run the specs, start a local mysql instance.  The default user is\r\nroot with an empty password.  Override the user/password with\r\nenvironment variables `DB_USER` and `DB_PASSWORD`.  The test creates a\r\ndb called 'cstest'.\r\n\r\n## Contributing\r\n\r\n1. Fork it ( https://github.com/bkayser/cleansweep/fork )\r\n2. Create your feature branch (`git checkout -b my-new-feature`)\r\n3. Commit your changes (`git commit -am 'Add some feature'`)\r\n4. Push to the branch (`git push origin my-new-feature`)\r\n5. Create a new Pull Request\r\n\r\n## License and Copyright\r\n\r\nCopyright 2014 New Relic, Inc., and Bill Kayser\r\n\r\nCovered by the MIT [LICENSE](LICENSE.txt).\r\n\r\n### Credits\r\n\r\nThis was all inspired and informed by [Percona's `pt-archiver`\r\nscript](http://www.percona.com/doc/percona-toolkit/2.1/pt-archiver.html)\r\nwritten by Baron Schwartz.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}